{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datorseende - Kursprojekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import dlib\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from imutils import face_utils as fu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjects\n",
    "Ange användarprofiler som ansiktsigenkännaren känner till. Nu känner den endast igen Niklas och Walter. Om någon annan hoppar in i bilden kommer den känna igen den profilen som personen liknar mest. Här använde vi oss inte av confidence igenkännaren returnerar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['', 'Niklas', 'Walter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### face_detector(img)\n",
    "För ansiktsdetektorn bestämde vi oss att använda haarcascade_frontalface_default cascade:n. Vi jämförde med LBPH casacades (defaut och improved), men tyckte att haarcascade fungerade bättre. Annors är funktionen rätt systematiskt. Bilden man matar in konverteras till grayscale och matas in i detectMultiScale funktionen som detekterar alla ansikten i bilden. Ifall det inte finns några ansikten returnerar funktionen None och None. Två värden för att funktionen som ansiktsdetektorn körs i kräver själva ansiktsdatan och koordinaterna omkring ansiktet. Dvs då ett ansikte detekteras returnerar funktionen koordinaterna och ansiktet. Om man kör imshow på face[] så får man utskuren bild med enbart ansiktet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(img):\n",
    "    \n",
    "    # Image already in array-form. No need to cv.imread()\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv.CascadeClassifier('C:\\\\Users\\\\Niklas\\\\Anaconda3\\\\Library\\\\etc\\\\haarcascades\\\\haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Detecting faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    # If no faces are detected, return None, None\n",
    "    if (len(faces) == 0):\n",
    "        return None, None\n",
    "    \n",
    "    # Asssigning coordinates for face\n",
    "    (x, y, w, h) = faces[0]\n",
    "    \n",
    "    # Return image with only face and coordinates\n",
    "    return gray[y: y + h, x: x + w], faces[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare_data(data_folder_path)\n",
    "Denna funktion läser träningsdata för ansiktsigenkännaren. Vi har redan givit två profiler enligt vilka vi har sorterat bilderna i vår directory. Funktionen nedan lokaliserar först datan och sedan körs ansiktsdetektions-funktionen som vi förklarade ovan. Hittar den ett ansikte lägger den ansiktsdatan i den angivna array faces[] och antingen 1 eller 2 in i den angivna arrayn labels[]. I directoryn är profilerna angivna enlgigt s1, s2... etc (subject1.). För varje subject tar vi bort s:et från directorn och konvererar siffran och sparar den i labels arrayn. Om ansiktsdetektorn returnerar None, None lade vi in en if sats som berättar vilken bild det gäller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_folder_path):\n",
    "    \n",
    "    # Accessing training data\n",
    "    dirs = os.listdir(data_folder_path)\n",
    "    faces = []\n",
    "    labels = []\n",
    "    \n",
    "    # Looping through subjects\n",
    "    for dir_name in dirs:\n",
    "        \n",
    "        if not dir_name.startswith('s'):\n",
    "            continue\n",
    "            \n",
    "        label = int(dir_name.replace('s', ''))\n",
    "        subject_dir_path = data_folder_path + '\\\\' + dir_name\n",
    "        subject_images_names = os.listdir(subject_dir_path)\n",
    "        \n",
    "        # Looping through subject images\n",
    "        for image_name in subject_images_names:\n",
    "            \n",
    "            if image_name.startswith('.'):\n",
    "                continue\n",
    "                \n",
    "            image_path = subject_dir_path + '\\\\' + image_name\n",
    "            image = cv.imread(image_path)\n",
    "            \n",
    "            # CSI ACTION\n",
    "            cv.imshow('Training data...', image)\n",
    "            cv.waitKey(50)\n",
    "            \n",
    "            #Feeding images into the face_detector function\n",
    "            face, rect = face_detector(image)\n",
    "            \n",
    "            # If face detected, add to list\n",
    "            \n",
    "            if face is None:\n",
    "                print(dir_name, ': ', image_name)\n",
    "            if face is not None:\n",
    "                faces.append(face)\n",
    "                labels.append(label)\n",
    "                \n",
    "    cv.destroyAllWindows()\n",
    "    cv.waitKey(1)\n",
    "    cv.destroyAllWindows()\n",
    "    \n",
    "    return faces, labels\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing out data\n",
    "Här körs funktionen ovan. Vi ser att inget ansikte hittades från två bilder och att totala mängden ansikten och profiler var 53."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "s1 :  30.jpg\n",
      "s2 :  22.jpg\n",
      "Data has been prepared\n",
      "Number of faces:  53\n",
      "Number of labels:  53\n"
     ]
    }
   ],
   "source": [
    "print('Preparing data...')\n",
    "faces, labels = prepare_data('training_data')\n",
    "print('Data has been prepared')\n",
    "\n",
    "print('Number of faces: ', len(faces))\n",
    "print('Number of labels: ', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our data\n",
    "Här skapar vi en ansiktsigenkännare och matar in datan som vi fick då vi förberedde vår data i funktionen ovan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_recognizer = cv.face.LBPHFaceRecognizer_create()\n",
    "face_recognizer.train(faces, np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw_text(img, text, x, y)\n",
    "En simpel funktion för att lägga text i vår videofeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(img, text, x, y):\n",
    "    cv.putText(img, text, (x, y), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eye_tracker()\n",
    "I den slutliga funktionen lägger vi ihop hela kakan. Först anger vi några globala variabler. Cap för real-time video feed. Vi valde att skapa en skild face detector i denna funktion för att vi ville ha olika parametrar i detectMultiScale funktionen då vi kör video. Dessutom använde vi lbp kaskaden istället för haar.<br><br>\n",
    "\n",
    "I p lagrar vi datan för ansiktslandmärken som används efter att profilerna ansikten blivit igenkända.<br><br>\n",
    "\n",
    "d är en dictionary som används för att lagra koorinater av landärken kring ögonen. Koordinaterna updateras för varje frame och används för att räkna EAR värdet.<br><br>\n",
    "\n",
    "EAR värdet börjar vid 0. Enligt profilernas ögon var värdet på EAR omkring 0.30-0.40 då ögat var öppet. Som gränssnitt för en blinkning bestämde vi att 0.21 fungerade bra. Så då ögat är mellan 30-50% stängt anser vi det som en början av en blinking eller trötta ögon. som lägsta värdet i en blinking blev EAR ca 0.15. Då man setup:ar programmet lönar det sig att kolla att kamerans vinkel är 90grader mot synvinkeln för noggrannaste resultatet. Om t.ex huvudet lutar framåt pga trötthet kommer också värdet att sjunka. För EAR värdet tyckte vi att det räcker om ena ögat stängs. Blinkningar och trötthet brukar fungera symmetriskt.<br><br>\n",
    "\n",
    "PERCLOS vädet börjar vid 0 och börjar variera enligt EAR-värdet. Notera att PERCLOS-värdet först blir aktuellt efter ca 25 sekunder, då programmet har kört igenom 250 frames (ca 10 FPS)(Ingen startar knappast bilen i sömnen, så detta borde vara helt OK).<br><br>\n",
    "\n",
    "arrClosed är en array som i början fylls med 250 nollor. Då EAR värdet, som kalkyleras längre ned i funktionen, är mindre än 0.21 kommer en etta att läggas i arrayns 250e plats medan arrayns första värde raderas.\n",
    "\n",
    "arrOpen är en array som först fylls med en etta, varefter den matas in med 249 nollor. Vi ger den en etta för att arrOpen används som nämnare tillsammans med arrClosed i en beräkning. Nämnaren får inte vara 0... Då EAR värdet är över 0.21 matas en etta i slutet av arrayn, samtidigt som arrayns första värde raderas. I princip motsatsen till arrClosed.<br><br>\n",
    "\n",
    "PERCLOS räknas inom en tidsperiod på 250 frames enligt: sum(arrClosed) / ((sum(arrClosed) + sum(arrOpen)). PERCLOS tröskelvärdena definierade enligt https://www.researchgate.net/figure/DROWSINESS-LEVELS-BASED-ON-THE-PERCLOS-THRESHOLDS_tbl1_283018835. <br><br>\n",
    "\n",
    "PERCLOS och EAR är visualiserat i videon i realtid. Då PERCLOS når sina tröskelvärden kommer det att meddela trötthetsnivån i grön, gul och röd, beroende på hur trött användaren är.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_tracker():\n",
    "    \n",
    "    # Assigning VideoCapture()\n",
    "    cap = cv.VideoCapture(0)\n",
    "    \n",
    "    # Importing cascade and predictor\n",
    "    face_cascade = cv.CascadeClassifier('C:\\\\Users\\\\Niklas\\\\Anaconda3\\\\Library\\\\etc\\\\lbpcascades\\\\lbpcascade_frontalface.xml')\n",
    "    p = 'shape_predictor_68_face_landmarks.dat'\n",
    "    \n",
    "    # Detector for face landmarks\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(p)\n",
    "    \n",
    "    # Global variables\n",
    "    d = {}\n",
    "    EAR = 0\n",
    "    PERCLOS = 0\n",
    "    arrClosed = []\n",
    "    arrOpen = [1]\n",
    "    \n",
    "    for i in range(250):\n",
    "        arrClosed.append(0)\n",
    "        \n",
    "    for i in range(249):\n",
    "        arrOpen.append(0)\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Getting our webcam feed, flipping it horizontally and assigning a grayscale version\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv.flip(frame, +1)\n",
    "        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detecting faces for facial landmarks\n",
    "        faces = detector(gray, 0)\n",
    "        \n",
    "        # Detecting coordinates around face\n",
    "        face_coords = face_cascade.detectMultiScale(gray, 1.05, 5)\n",
    "        \n",
    "        # Loop to draw rectangle around faces and recognizing the user\n",
    "        for (x, y, w, h) in face_coords:\n",
    "            \n",
    "            # Draw rectangle and assign coordinates around face\n",
    "            cv.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            roi_gray = gray[y: y + h, x: x + w]\n",
    "            roi_color = frame[y: y + h, x: x + w]\n",
    "            \n",
    "            # Using our face_recognizer that we trained earlier\n",
    "            label = face_recognizer.predict(roi_gray)\n",
    "            label_text = subjects[label[0]]\n",
    "            \n",
    "            # Adding name of recognized person\n",
    "            draw_text(frame, label_text, x, y - 5)\n",
    "        \n",
    "        # Loop for landmarks\n",
    "        for face in faces:\n",
    "            \n",
    "            # Shape of face\n",
    "            shape = fu.shape_to_np(predictor(gray, face))\n",
    "            \n",
    "            # Loop to draw landmarks\n",
    "            for i, (x, y) in enumerate(shape, 1):\n",
    "                \n",
    "                # Only drawing eyes\n",
    "                if i >= 37 and i <= 48:\n",
    "                    cv.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "                    \n",
    "                    # Assigning landmark coordinates in a dictionary\n",
    "                    d['coord{0}'.format(i)] = (x, y)\n",
    "\n",
    "            # Preparing coordinates for EAR calculation. earV values for vertical distances. earH for horizontal\n",
    "            earV1 = (abs(d['coord44'][0] - d['coord48'][0]), abs(d['coord44'][1] - d['coord48'][1]))\n",
    "            earV2 = (abs(d['coord45'][0] - d['coord47'][0]), abs(d['coord45'][1] - d['coord47'][1]))\n",
    "            earH = (abs(d['coord43'][0] - d['coord46'][0]), abs(d['coord43'][1] - d['coord46'][1]))\n",
    "            \n",
    "            earV = (earV1[0] + earV2[0], earV1[1] + earV2[1])\n",
    "            \n",
    "            # Final EAR-formula\n",
    "            EAR = earV[1] / (2 * earH[0])\n",
    "            \n",
    "            # Threshold for open/closed eye. If EAR < 0.21, the eye is almost closed.\n",
    "            # Appending 1 to closedFrames and removing first item in list. Timeframe of 250 frames at 10 fps\n",
    "            # Appending 0 to openFrames and removing first item in list. \n",
    "            # Couldn't get append and pop to work properly on one line\n",
    "            if EAR <= 0.21:\n",
    "                arrClosed.append(1)\n",
    "                arrClosed.pop(0)\n",
    "                arrOpen.append(0)\n",
    "                arrOpen.pop(0)\n",
    "                \n",
    "\n",
    "            # Doing the opposite\n",
    "            if EAR > 0.21:\n",
    "                arrClosed.append(0)\n",
    "                arrClosed.pop(0)\n",
    "                arrOpen.append(1)\n",
    "                arrOpen.pop(0)\n",
    "            \n",
    "            \n",
    "            # Calculating the percentage of time eyes are closed\n",
    "            PERCLOS = sum(arrClosed) / (sum(arrClosed) + sum(arrOpen))\n",
    "            \n",
    "\n",
    "        cv.putText(frame, ('EAR: ' + (str(round(EAR, 2)))), (15, 30), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2) \n",
    "        cv.putText(frame, ('PERCLOS: ' + (str(round(PERCLOS * 100, 3))) + '%'), (15, 55), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "        \n",
    "        # Some PERCLOS thresholds.\n",
    "        if PERCLOS >= 0.15:\n",
    "            cv.putText(frame, ('SEVERE DROWSINESS'), (15, 80), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)    \n",
    "        if PERCLOS >= 0.07 and PERCLOS < 0.15:\n",
    "            cv.putText(frame, ('MODERATE DROWSINESS'), (15, 80), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 255), 2)\n",
    "        if PERCLOS < 0.07:\n",
    "            cv.putText(frame, ('LOW DROWSINESS'), (15, 80), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "            \n",
    "        \n",
    "        cv.imshow('Frame', frame)\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            break   \n",
    "            \n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "    \n",
    "#     print(closedFrames + openFrames)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eye_tracker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
